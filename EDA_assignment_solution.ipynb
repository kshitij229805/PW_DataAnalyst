{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a1ca3cd",
      "metadata": {
        "id": "5a1ca3cd"
      },
      "source": [
        "# Foundations of Machine Learning and EDA – Assignment Solutions\n",
        "\n",
        "\n",
        "This notebook contains detailed answers and code for Questions 1–10. Question numbers and wording follow the original assignment exactly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca77f0f",
      "metadata": {
        "id": "cca77f0f"
      },
      "source": [
        "## Question 1\n",
        "**What is the difference between AI, ML, DL, and Data Science? Provide a brief explanation of each.**  \n",
        "_(Hint: Compare their scope, techniques, and applications for each.)_\n",
        "\n",
        "### Answer\n",
        "We can think of these terms as concentric circles:\n",
        "\n",
        "1. **Artificial Intelligence (AI)**  \n",
        "- **Scope:** The broadest field. AI is about making machines behave intelligently – able to **perceive, reason, learn, and act**.  \n",
        "- **Techniques:** Rule‑based systems, search algorithms, planning, expert systems, logic, as well as machine learning and deep learning.  \n",
        "- **Applications:** Game playing (e.g., chess engines), recommendation systems, chatbots, robots, self‑driving cars, fraud detection, etc.  \n",
        "- **Real‑life example:** A customer support chatbot that understands queries, asks follow‑up questions, and decides whether to escalate to a human.\n",
        "\n",
        "2. **Machine Learning (ML)**  \n",
        "- **Scope:** A **subset of AI** focused specifically on algorithms that **learn patterns from data** instead of relying only on explicit rules.  \n",
        "- **Idea:** \"Learn from data\" → given examples \\((x, y)\\), learn a function \\(f(x)\\) that predicts \\(y\\) for new inputs.  \n",
        "- **Techniques:** Supervised learning (regression, classification), unsupervised learning (clustering, dimensionality reduction), reinforcement learning, etc.  \n",
        "- **Applications:** Email spam detection, credit‑card fraud detection, demand forecasting, house price prediction.  \n",
        "- **Example:** A model that predicts whether a loan applicant will default based on their past financial history. The rules are not hard‑coded; they are *learned* from historical data.\n",
        "\n",
        "3. **Deep Learning (DL)**  \n",
        "- **Scope:** A **subset of machine learning** that uses **deep neural networks** (many layers of non‑linear transformations).  \n",
        "- **Idea:** Automatically learn **hierarchical representations** (e.g., edges → shapes → objects in images).  \n",
        "- **Techniques:** Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTMs, Transformers, autoencoders, GANs, etc.  \n",
        "- **Applications:** Image recognition (face unlock on phones), speech recognition (voice assistants), machine translation, large language models (e.g., ChatGPT‑like systems).  \n",
        "- **Example:** A CNN trained to classify X‑ray images into \"normal\" and \"pneumonia\" using millions of labeled images.\n",
        "\n",
        "4. **Data Science**  \n",
        "- **Scope:** A broader **end‑to‑end discipline** that involves **collecting, cleaning, exploring, modeling, and communicating insights from data**. ML/DL are tools within data science.  \n",
        "- **Tasks:** Data collection, data cleaning, Exploratory Data Analysis (EDA), feature engineering, model building (using ML/DL), model evaluation, visualization, storytelling, and deployment support.  \n",
        "- **Techniques:** Statistics, ML/DL, data visualization, SQL, programming (Python/R), dashboards, A/B testing, etc.  \n",
        "- **Applications:** Business analytics, product analytics, user behavior analysis, marketing campaigns, operations optimization.  \n",
        "- **Example:** A data scientist at an e‑commerce company uses SQL to pull user data, cleans it, explores patterns in purchases, builds a churn prediction model using ML, and then creates a dashboard for business stakeholders.\n",
        "\n",
        "### Summary of differences\n",
        "- **AI** – Big umbrella: making machines intelligent (can include both rule‑based and learning‑based systems).\n",
        "- **ML** – Subset of AI: algorithms that *learn from data* (e.g., decision trees, SVM, linear regression).\n",
        "- **DL** – Subset of ML: uses deep neural networks, works very well with large data (images, text, audio).\n",
        "- **Data Science** – Practical discipline that uses statistics, ML/DL, and domain knowledge to solve real‑world data problems and communicate insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f2a3fa",
      "metadata": {
        "id": "58f2a3fa"
      },
      "source": [
        "## Question 2\n",
        "**Explain overfitting and underfitting in ML. How can you detect and prevent them?**  \n",
        "_Hint: Discuss bias‑variance tradeoff, cross‑validation, and regularization techniques._\n",
        "\n",
        "### Answer\n",
        "When we train a model, we want it to perform well not only on the **training data** but also on **unseen test data**. Overfitting and underfitting are two common problems:\n",
        "\n",
        "#### Underfitting\n",
        "- A model is **too simple** to capture the underlying pattern in the data.\n",
        "- It performs **poorly on both training and test data** (high error everywhere).\n",
        "- Example: Fitting a straight line to data that clearly follows a curved (quadratic) pattern.\n",
        "\n",
        "\n",
        "#### Overfitting\n",
        "- A model is **too complex** and **memorizes** noise and random fluctuations in the training data.\n",
        "- It shows **very low training error but high test error**.\n",
        "- Example: A decision tree that grows very deep and perfectly classifies each training sample but fails badly on new samples.\n",
        "\n",
        "#### Bias‑Variance Trade‑off\n",
        "- **Bias**: Error due to overly simplified assumptions in the model (e.g., assuming linear relationship when it's not). High bias → underfitting.  \n",
        "- **Variance**: Error due to model's sensitivity to small fluctuations in the training data (too complex, changes a lot when data changes). High variance → overfitting.  \n",
        "- We want a balance: **not too simple, not too complex**.\n",
        "\n",
        "#### Detecting Overfitting and Underfitting\n",
        "1. **Train–Test (or Train–Validation) Performance Comparison**  \n",
        "- Underfitting: Training accuracy is low, validation/test accuracy is also low.  \n",
        "- Overfitting: Training accuracy is very high, but validation/test accuracy is much lower.\n",
        "\n",
        "2. **Learning Curves** (plot error vs. training set size)\n",
        "- Underfitting: Both training and validation errors are high and close to each other.  \n",
        "- Overfitting: Training error is low, validation error is high; sometimes validation error decreases with more data.\n",
        "\n",
        "3. **Cross‑Validation**  \n",
        "- Use **k‑fold cross‑validation** to estimate how the model performs on unseen data.  \n",
        "- If performance varies a lot across folds or is much worse than training performance → likely overfitting.\n",
        "\n",
        "#### Preventing Overfitting\n",
        "1. **Simplify the model**  \n",
        "- Use fewer features or a simpler algorithm (e.g., shallow tree instead of very deep tree).\n",
        "\n",
        "2. **Regularization**  \n",
        "- Add a **penalty** on large weights in linear/logistic regression, neural networks, etc.  \n",
        "- **L2 (Ridge)** regularization: adds \\( \\lambda \\sum w_i^2 \\).  \n",
        "- **L1 (Lasso)** regularization: adds \\( \\lambda \\sum |w_i| \\), can drive some weights to zero (feature selection).\n",
        "\n",
        "3. **Early Stopping**  \n",
        "- In iterative training (like gradient descent, neural networks), monitor validation loss and stop when it starts increasing.\n",
        "\n",
        "4. **More Training Data**  \n",
        "- With more diverse data, the model is less likely to memorize noise.\n",
        "\n",
        "5. **Data Augmentation** (for images/text/audio)\n",
        "- E.g., rotate/flip images, add noise, etc., to increase data variability.\n",
        "\n",
        "**Real‑life example:**  \n",
        "Imagine predicting house prices. If we fit a model using only one feature (e.g., number of bedrooms), it may **underfit** because it ignores other important features (location, area). If we build a very complex model that tries to perfectly match every training house price including outliers, it may **overfit** and fail on new houses.\n",
        "\n",
        "#### Preventing Underfitting\n",
        "- Use a **more expressive model** (e.g., move from linear to polynomial regression, increase tree depth slightly).  \n",
        "- Add **relevant features** or better feature engineering.  \n",
        "- Reduce regularization strength if it is too high.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbdb844a",
      "metadata": {
        "id": "cbdb844a"
      },
      "source": [
        "## Question 3\n",
        "**How would you handle missing values in a dataset? Explain at least three methods with examples.**  \n",
        "_Hint: Consider deletion, mean/median imputation, and predictive modeling._\n",
        "\n",
        "### Answer\n",
        "Real‑world datasets (customer data, medical records, sensor logs, surveys, etc.) almost always have **missing values**. Handling them properly is important because many ML algorithms cannot work directly with missing entries.\n",
        "\n",
        "Let us consider a simple example dataset of customers:\n",
        "\n",
        "| Customer | Age | Income | City     |\n",
        "|----------|-----|--------|----------|\n",
        "| A        | 25  | 50k    | Delhi    |\n",
        "| B        | NaN | 60k    | Mumbai   |\n",
        "| C        | 35  | NaN    | Delhi    |\n",
        "| D        | 40  | 80k    | NaN      |\n",
        "\n",
        "We will discuss three common strategies:\n",
        "\n",
        "#### 1. Deletion (Dropping Rows or Columns)\n",
        "- **Listwise deletion (drop rows):** Remove rows with missing values.  \n",
        "- **Column deletion:** Remove columns that have too many missing values.\n",
        "\n",
        "**When to use:**\n",
        "- When the amount of missing data is **small** (e.g., < 5%).  \n",
        "- When removed rows are unlikely to introduce bias.\n",
        "\n",
        "**Example:**  \n",
        "In the above table, if only customer B has a missing Age, we might drop that row if the dataset is large and one record does not matter much.\n",
        "\n",
        "In code (pandas):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6684ac",
      "metadata": {
        "id": "2b6684ac"
      },
      "outputs": [],
      "source": [
        "# Example: Deletion of rows with missing values\n",
        "import pandas as pd\n",
        "\n",
        "# Tiny demo dataset\n",
        "customers = pd.DataFrame({\n",
        "    'Customer': ['A', 'B', 'C', 'D'],\n",
        "    'Age': [25, None, 35, 40],\n",
        "    'Income': [50000, 60000, None, 80000],\n",
        "    'City': ['Delhi', 'Mumbai', 'Delhi', None]\n",
        "})\n",
        "\n",
        "print(\"Original DataFrame:\\n\", customers)\n",
        "\n",
        "# Drop rows with any missing value\n",
        "customers_dropped = customers.dropna()\n",
        "print(\"\\nAfter dropping rows with missing values:\\n\", customers_dropped)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4831d0a",
      "metadata": {
        "id": "d4831d0a"
      },
      "source": [
        "#### 2. Mean/Median/Mode Imputation\n",
        "- For **numerical features**, we can fill missing values with:  \n",
        "  - **Mean** (average) – sensitive to outliers.  \n",
        "  - **Median** – more robust when data is skewed.  \n",
        "- For **categorical features**, we often use **mode** (most frequent category).\n",
        "\n",
        "**Example (Age):**  \n",
        "Suppose Ages are [25, 30, 35, 40] and one Age is missing.  \n",
        "- Mean Age = (25 + 30 + 35 + 40) / 4 = 32.5 → use 32.5.  \n",
        "- Median Age = (30 + 35) / 2 = 32.5 (in this particular case).  \n",
        "In skewed income distributions, median is usually better than mean.\n",
        "\n",
        "In code (pandas):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550ccfa3",
      "metadata": {
        "id": "550ccfa3"
      },
      "outputs": [],
      "source": [
        "# Mean/median/mode imputation example\n",
        "import pandas as pd\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    'Customer': ['A', 'B', 'C', 'D'],\n",
        "    'Age': [25, None, 35, 40],\n",
        "    'Income': [50000, 60000, None, 80000],\n",
        "    'City': ['Delhi', 'Mumbai', 'Delhi', None]\n",
        "})\n",
        "\n",
        "# Fill numerical columns\n",
        "customers['Age'] = customers['Age'].fillna(customers['Age'].mean())\n",
        "customers['Income'] = customers['Income'].fillna(customers['Income'].median())\n",
        "\n",
        "# Fill categorical column with mode\n",
        "customers['City'] = customers['City'].fillna(customers['City'].mode()[0])\n",
        "\n",
        "print(customers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a21357a8",
      "metadata": {
        "id": "a21357a8"
      },
      "source": [
        "#### 3. Predictive Modeling (Model‑based Imputation)\n",
        "- Here we **train a model** to predict missing values based on other features.  \n",
        "- For example, if Age is missing but we have Income, City, and Spending Score, we can train a regression model to predict Age.\n",
        "\n",
        "**Steps:**\n",
        "1. Separate rows where the target feature (e.g., Age) is **known**.  \n",
        "2. Train a model (e.g., linear regression, decision tree, KNN) to predict Age from other features.  \n",
        "3. Use this model to predict missing Age values.\n",
        "\n",
        "**Real‑life example:**  \n",
        "In a hospital database, some patients' blood pressure readings might be missing. Instead of dropping patients or filling with a simple average, we can use other variables (age, weight, previous readings, medications) to predict blood pressure more accurately.\n",
        "\n",
        "In sklearn, we can use `KNNImputer` or `IterativeImputer` for more advanced imputation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad0fdbb6",
      "metadata": {
        "id": "ad0fdbb6"
      },
      "outputs": [],
      "source": [
        "# Skeleton example using KNNImputer (note: small demo, not real data)\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "X = np.array([\n",
        "    [25, 50000],\n",
        "    [np.nan, 60000],\n",
        "    [35, np.nan],\n",
        "    [40, 80000]\n",
        "])\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "print(X_imputed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74be8738",
      "metadata": {
        "id": "74be8738"
      },
      "source": [
        "Other methods include **forward/backward fill** for time series, and **using special categories** like \"Unknown\" for some categorical variables. Choice of method depends on data size, missingness pattern, and domain knowledge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eaa5dce",
      "metadata": {
        "id": "7eaa5dce"
      },
      "source": [
        "## Question 4\n",
        "**What is an imbalanced dataset? Describe two techniques to handle it (theoretical + practical).**  \n",
        "_Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models._\n",
        "\n",
        "### Answer\n",
        "#### What is an imbalanced dataset?\n",
        "- A dataset is **imbalanced** when the **class distribution is highly skewed** – one class has many more examples than another.  \n",
        "- Example: In fraud detection, 99.5% transactions are normal, 0.5% are fraudulent.  \n",
        "- If we train a normal classifier, it might predict **\"not fraud\" for everything** and still get 99.5% accuracy → but it is useless for detecting fraud.\n",
        "\n",
        "Evaluation metrics like **accuracy** can be misleading; we use **precision, recall, F1‑score, ROC‑AUC**, etc.\n",
        "\n",
        "We’ll describe two techniques:\n",
        "\n",
        "### Technique 1: Random Over/Under Sampling\n",
        "1. **Random Oversampling**  \n",
        "- **Idea:** Increase the number of minority class samples by **duplicating** them (or by generating new synthetic variants).  \n",
        "- Pros: Balances classes; simple.  \n",
        "- Cons: Can lead to overfitting on duplicated examples.\n",
        "\n",
        "2. **Random Undersampling**  \n",
        "- **Idea:** Reduce the majority class by **removing some examples**.  \n",
        "- Pros: Smaller dataset, faster training.  \n",
        "- Cons: May lose useful information (if you drop informative majority samples).\n",
        "\n",
        "**Real‑life example:**  \n",
        "In an email spam dataset with 90% non‑spam and 10% spam, we can oversample spam emails (by duplicating them) so the classifier pays more attention to the spam class.\n",
        "\n",
        "In code (using `sklearn.utils.resample` for simple oversampling):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09745996",
      "metadata": {
        "id": "09745996"
      },
      "outputs": [],
      "source": [
        "# Simple demo of random oversampling for a binary class\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Dummy imbalanced dataset\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6],\n",
        "    'label':    [0, 0, 0, 0, 1, 1]\n",
        "})\n",
        "\n",
        "print(\"Original class distribution:\\n\", data['label'].value_counts())\n",
        "\n",
        "majority = data[data['label'] == 0]\n",
        "minority = data[data['label'] == 1]\n",
        "\n",
        "minority_upsampled = resample(minority,\n",
        "                              replace=True,\n",
        "                              n_samples=len(majority),\n",
        "                              random_state=42)\n",
        "\n",
        "upsampled = pd.concat([majority, minority_upsampled])\n",
        "print(\"\\nAfter oversampling:\\n\", upsampled['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e4828d5",
      "metadata": {
        "id": "5e4828d5"
      },
      "source": [
        "### Technique 2: SMOTE (Synthetic Minority Over‑sampling Technique)\n",
        "- **Idea:** Instead of simply duplicating minority samples, SMOTE **creates synthetic samples** by interpolating between existing minority samples.  \n",
        "- For a minority point, SMOTE picks one of its k nearest minority neighbors and generates a new sample along the line joining them.  \n",
        "- This helps the classifier learn a **smoother decision boundary**.\n",
        "\n",
        "**Theoretical intuition:**  \n",
        "By creating new, slightly varied minority points, the decision region for the minority class is expanded in feature space, making it easier for the model to learn.\n",
        "\n",
        "In practice, we can use `imblearn.over_sampling.SMOTE` (from the `imbalanced-learn` library):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98eea2bb",
      "metadata": {
        "id": "98eea2bb"
      },
      "outputs": [],
      "source": [
        "# Skeleton code for SMOTE (will work when imblearn is installed)\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "# print(\"New class distribution:\", pd.Series(y_resampled).value_counts())\n",
        "\n",
        "print(\"SMOTE example placeholder – uncomment when imblearn is available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b242df",
      "metadata": {
        "id": "42b242df"
      },
      "source": [
        "### Technique 3 (brief mention): Class Weights\n",
        "- Many models (e.g., logistic regression, SVM, tree‑based models) allow you to specify **class weights**.  \n",
        "- Assign higher weight to the minority class so that **misclassifying it is penalized more** in the loss function.  \n",
        "- In scikit‑learn, `class_weight='balanced'` automatically adjusts weights inversely proportional to class frequencies.\n",
        "\n",
        "**Real‑life example:**  \n",
        "In medical diagnosis for a rare disease, classifying a sick patient as healthy is very costly. By increasing the weight of the \"disease\" class in the loss, the model focuses more on correctly identifying diseased patients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5dce24",
      "metadata": {
        "id": "cb5dce24"
      },
      "source": [
        "## Question 5\n",
        "**Why is feature scaling important in ML? Compare Min‑Max scaling and Standardization.**  \n",
        "_Hint: Explain impact on distance‑based algorithms (e.g., KNN, SVM) and gradient descent._\n",
        "\n",
        "### Answer\n",
        "Different features in a dataset can be on very different scales. For example:\n",
        "\n",
        "- Age: 18–70  \n",
        "- Salary: 20,000–2,00,000  \n",
        "- Number of purchases: 0–100\n",
        "\n",
        "Many ML algorithms are **sensitive to the scale** of features. If we do not scale them, features with **larger numeric ranges dominate** distance calculations and gradients.\n",
        "\n",
        "#### Why feature scaling is important\n",
        "1. **Distance‑based algorithms (KNN, K‑Means, SVM with RBF kernel)**  \n",
        "- These use Euclidean distance or similar metrics.  \n",
        "- If one feature has a much larger range than others, it will **dominate the distance** measure.  \n",
        "- Example: In KNN, if we use Age and Salary, and Salary is in lakhs, then salary differences overshadow age differences unless we scale.\n",
        "\n",
        "2. **Gradient Descent‑based algorithms (Linear Regression, Logistic Regression, Neural Networks)**  \n",
        "- Features on different scales can cause the **loss surface to be very elongated**, leading to slow or unstable convergence.  \n",
        "- Scaling makes gradient descent **more stable and faster**.\n",
        "\n",
        "3. **Regularization (L1/L2)**  \n",
        "- When features are on similar scales, regularization treats them more fairly.\n",
        "\n",
        "#### Min‑Max Scaling (Normalization)\n",
        "- **Formula:**  \n",
        "  \\[\n",
        "  x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
        "  \\]\n",
        "- Transforms values to a fixed range, usually **[0, 1]**.  \n",
        "- Useful when we **know the min and max** and want a bounded scale.\n",
        "\n",
        "**Example:**  \n",
        "If Age ranges from 18 to 60, and a person is 30 years old:\n",
        "\\( x_{scaled} = (30 - 18) / (60 - 18) = 12 / 42 \\approx 0.2857. \\)\n",
        "\n",
        "#### Standardization (Z‑score Scaling)\n",
        "- **Formula:**  \n",
        "  \\[\n",
        "  x_{scaled} = \\frac{x - \\mu}{\\sigma}\n",
        "  \\]\n",
        "  where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.\n",
        "- Resulting feature typically has **mean 0** and **standard deviation 1**.  \n",
        "- Values are not bounded to [0, 1]; they can be negative or greater than 1.\n",
        "\n",
        "**Example:**  \n",
        "If mean Age is 35 and standard deviation is 10, then Age = 30 →  \n",
        "\\( x_{scaled} = (30 - 35)/10 = -0.5. \\)\n",
        "\n",
        "#### When to use which?\n",
        "- **Min‑Max Scaling**  \n",
        "  - Useful for algorithms that expect inputs in a **bounded range**, e.g., neural networks with sigmoid activation, or when you want to preserve **exact 0–1 ranges**.  \n",
        "  - Can be sensitive to **outliers** (min and max can be skewed).\n",
        "\n",
        "- **Standardization**  \n",
        "  - Preferred in many ML algorithms (SVM, logistic regression, linear regression, PCA).  \n",
        "  - More robust when data approximately follows a **Gaussian distribution**.  \n",
        "  - Outliers still affect mean and std, but not as extremely as min‑max.\n",
        "\n",
        "In scikit‑learn, we use `MinMaxScaler` and `StandardScaler`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b3d3f99",
      "metadata": {
        "id": "9b3d3f99"
      },
      "outputs": [],
      "source": [
        "# Example: Min-Max scaling vs Standardization\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "X = pd.DataFrame({\n",
        "    'Age': [18, 25, 40, 60],\n",
        "    'Salary': [20000, 50000, 120000, 200000]\n",
        "})\n",
        "\n",
        "mms = MinMaxScaler()\n",
        "ssc = StandardScaler()\n",
        "\n",
        "X_minmax = mms.fit_transform(X)\n",
        "X_std = ssc.fit_transform(X)\n",
        "\n",
        "print(\"Original:\\n\", X)\n",
        "print(\"\\nMin-Max scaled:\\n\", X_minmax)\n",
        "print(\"\\nStandardized:\\n\", X_std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f017b03e",
      "metadata": {
        "id": "f017b03e"
      },
      "source": [
        "## Question 6\n",
        "**Compare Label Encoding and One‑Hot Encoding. When would you prefer one over the other?**  \n",
        "_Hint: Consider categorical variables with ordinal vs. nominal relationships._\n",
        "\n",
        "### Answer\n",
        "Categorical variables (e.g., City, Color, Education Level) must be converted into numerical form before feeding into most ML algorithms.\n",
        "\n",
        "#### 1. Label Encoding\n",
        "- **Idea:** Assign an **integer label** to each category.\n",
        "- Example: City = {Delhi, Mumbai, Chennai}  \n",
        "  - Delhi → 0  \n",
        "  - Mumbai → 1  \n",
        "  - Chennai → 2\n",
        "\n",
        "- Pros: Simple, uses only **one column**, good for **tree‑based models** (Decision Trees, Random Forests, XGBoost) that can handle integer codes as categories.\n",
        "- Cons: Imposes an **artificial ordering** (0 < 1 < 2), which may not make sense for **nominal** categories. Algorithms that rely on numerical distance (e.g., KNN, linear models) may misinterpret the labels.\n",
        "\n",
        "#### 2. One‑Hot Encoding\n",
        "- **Idea:** Create a **binary column** for each category.  \n",
        "- Example: City = {Delhi, Mumbai, Chennai} → three columns:  \n",
        "  - City_Delhi, City_Mumbai, City_Chennai  \n",
        "  - (1, 0, 0) for Delhi, (0, 1, 0) for Mumbai, etc.\n",
        "- Pros: Does **not impose any ordering**, works well for **nominal** (unordered) categories.  \n",
        "- Cons: Increases dimensionality (can be large when there are many categories → \"curse of dimensionality\").\n",
        "\n",
        "#### Ordinal vs Nominal\n",
        "- **Ordinal categories:** Have a meaningful order (e.g., Education Level: Primary < Secondary < Graduate < Postgraduate).  \n",
        "- **Nominal categories:** No natural order (e.g., City names, Colors, Product IDs).\n",
        "\n",
        "#### When to prefer which?\n",
        "- Use **Label Encoding** when:\n",
        "  - The categories are **ordinal** (order matters).  \n",
        "  - Or when using **tree‑based models**, which can handle label‑encoded nominal categorical variables reasonably well.\n",
        "\n",
        "- Use **One‑Hot Encoding** when:\n",
        "  - The categories are **nominal** and will be used in **distance‑based or linear models** (KNN, SVM, linear/logistic regression).  \n",
        "  - You want to avoid misleading the model with artificial ordinal relationships.\n",
        "\n",
        "**Real‑life examples:**\n",
        "- Customer \"Membership Level\" (Bronze, Silver, Gold, Platinum) → **label encoding** with appropriate ordering is fine.  \n",
        "- Customer \"Country\" or \"City\" → **one‑hot encoding** is safer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d5ea1a",
      "metadata": {
        "id": "f5d5ea1a"
      },
      "outputs": [],
      "source": [
        "# Example: Label Encoding vs One-Hot Encoding\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "cities = pd.DataFrame({'City': ['Delhi', 'Mumbai', 'Chennai', 'Delhi']})\n",
        "\n",
        "# Label Encoding\n",
        "le = LabelEncoder()\n",
        "cities['City_Label'] = le.fit_transform(cities['City'])\n",
        "\n",
        "# One-Hot Encoding\n",
        "cities_ohe = pd.get_dummies(cities['City'], prefix='City')\n",
        "\n",
        "print(\"With label encoding:\\n\", cities)\n",
        "print(\"\\nWith one-hot encoding:\\n\", cities_ohe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5df290fc",
      "metadata": {
        "id": "5df290fc"
      },
      "source": [
        "## Question 7 – Google Play Store Dataset\n",
        "**(a) Analyze the relationship between app categories and ratings. Which categories have the highest/lowest average ratings, and what could be the possible reasons?**  \n",
        "Dataset: `https://github.com/MasteriNeuron/datasets.git`  \n",
        "_(Include your Python code and output in the code box below.)_\n",
        "\n",
        "### Plan\n",
        "1. Load the Google Play Store dataset (after downloading/cloning it locally).  \n",
        "2. Clean the data: remove missing or invalid ratings/categories.  \n",
        "3. Group by `Category` and compute the **average rating** per category.  \n",
        "4. Sort categories by their mean rating to find the highest/lowest ones.  \n",
        "5. Interpret possible reasons (e.g., game apps may get more extreme ratings, utility apps may have more stable moderate ratings, etc.).\n",
        "\n",
        "> **Note:** In this notebook, we show code using placeholder file paths. When running on your machine, replace the path with the actual CSV path from the cloned repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40eba5f6",
      "metadata": {
        "id": "40eba5f6"
      },
      "outputs": [],
      "source": [
        "# Google Play Store analysis – skeleton code\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Replace this path with the actual file path after cloning the repo\n",
        "# Example: df = pd.read_csv('datasets/googleplaystore.csv')\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('googleplaystore.csv')  # Adjust path as needed\n",
        "    print(\"Loaded googleplaystore.csv successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(\"googleplaystore.csv not found. Please update the file path.\")\n",
        "    # Create a small dummy DataFrame to demonstrate the analysis steps\n",
        "    df = pd.DataFrame({\n",
        "        'Category': ['GAME', 'GAME', 'TOOLS', 'TOOLS', 'EDUCATION', 'EDUCATION'],\n",
        "        'Rating':   [4.5, 4.7, 4.0, 3.8, 4.8, 4.6]\n",
        "    })\n",
        "\n",
        "# Basic cleaning: drop rows with missing Category or Rating\n",
        "df_clean = df.dropna(subset=['Category', 'Rating'])\n",
        "\n",
        "# Group by category and compute mean rating\n",
        "category_ratings = df_clean.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n",
        "\n",
        "print(\"Average rating by category (descending):\\n\", category_ratings.head(10))\n",
        "print(\"\\nLowest-rated categories:\\n\", category_ratings.tail(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db69d60d",
      "metadata": {
        "id": "db69d60d"
      },
      "source": [
        "### Interpretation (example reasoning)\n",
        "- Categories with **higher average ratings** (e.g., EDUCATION, HEALTH & FITNESS, PRODUCTIVITY) often provide **clear utility or learning value**, so satisfied users tend to rate them highly.  \n",
        "- Categories with **lower average ratings** (e.g., some GAME subcategories or COMMUNICATION) may suffer from **bugs, ads, battery drain, or network issues**, leading to user frustration.  \n",
        "- Games often receive **polarized ratings** – some users love them, others dislike ads/in‑app purchases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b22b706",
      "metadata": {
        "id": "8b22b706"
      },
      "source": [
        "## Question 8 – Titanic Dataset\n",
        "**(a)** Compare the survival rates based on passenger class (`Pclass`). Which class had the highest survival rate, and why do you think that happened?  \n",
        "**(b)** Analyze how age (`Age`) affected survival. Group passengers into children (Age < 18) and adults (Age ≥ 18). Did children have a better chance of survival?  \n",
        "Dataset: `https://github.com/MasteriNeuron/datasets.git`  \n",
        "_(Include your Python code and output in the code box below.)_\n",
        "\n",
        "### Plan\n",
        "1. Load the Titanic dataset (e.g., `titanic.csv`).  \n",
        "2. Clean data (remove or impute missing Age).  \n",
        "3. For (a): group by `Pclass` and compute survival rate = mean of `Survived`.  \n",
        "4. For (b): create a new column `AgeGroup` = 'Child' if Age < 18 else 'Adult', then compute survival rates by `AgeGroup`.  \n",
        "5. Interpret results: historically, higher‑class passengers and children often had better access to lifeboats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a6ecd7",
      "metadata": {
        "id": "24a6ecd7"
      },
      "outputs": [],
      "source": [
        "# Titanic analysis – skeleton code\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Replace with actual path, e.g., 'datasets/titanic.csv'\n",
        "try:\n",
        "    titanic = pd.read_csv('titanic.csv')\n",
        "    print(\"Loaded titanic.csv successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(\"titanic.csv not found. Using dummy data for demonstration.\")\n",
        "    titanic = pd.DataFrame({\n",
        "        'Pclass':   [1, 1, 2, 2, 3, 3],\n",
        "        'Survived': [1, 0, 1, 0, 0, 1],\n",
        "        'Age':      [5, 38, 17, 25, 30, 10]\n",
        "    })\n",
        "\n",
        "# Drop rows with missing Age for the Age analysis\n",
        "titanic_age = titanic.dropna(subset=['Age'])\n",
        "\n",
        "# (a) Survival rate by Pclass\n",
        "survival_by_class = titanic_age.groupby('Pclass')['Survived'].mean()\n",
        "print(\"Survival rate by Pclass:\\n\", survival_by_class)\n",
        "\n",
        "# (b) Children vs Adults\n",
        "titanic_age['AgeGroup'] = titanic_age['Age'].apply(lambda x: 'Child' if x < 18 else 'Adult')\n",
        "\n",
        "survival_by_agegroup = titanic_age.groupby('AgeGroup')['Survived'].mean()\n",
        "print(\"\\nSurvival rate by AgeGroup:\\n\", survival_by_agegroup)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afff825b",
      "metadata": {
        "id": "afff825b"
      },
      "source": [
        "### Interpretation (typical result)\n",
        "- Historically, **1st class** passengers had the highest survival rate, followed by 2nd class, then 3rd class. Reasons include:  \n",
        "  - First‑class cabins were closer to lifeboats.  \n",
        "  - Social status and crew prioritization.  \n",
        "  - Better access to information about the emergency.\n",
        "- For age groups, **children generally had higher survival rates** than adults due to the \"women and children first\" policy during evacuation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6becb5fd",
      "metadata": {
        "id": "6becb5fd"
      },
      "source": [
        "## Question 9 – Flight Price Prediction Dataset\n",
        "**(a)** How do flight prices vary with the days left until departure? Identify any exponential price surges and recommend the best booking window.  \n",
        "**(b)** Compare prices across airlines for the same route (e.g., Delhi‑Mumbai). Which airlines are consistently cheaper/premium, and why?  \n",
        "Dataset: `https://github.com/MasteriNeuron/datasets.git`  \n",
        "_(Include your Python code and output in the code box below.)_\n",
        "\n",
        "### Plan\n",
        "- Assume the dataset has columns like `price`, `days_left`, `airline`, `source`, `destination`, etc.  \n",
        "- (a) Plot `price` vs. `days_left` and look for a pattern (often prices are lower when booked several days/weeks in advance, then surge close to departure).  \n",
        "- (b) Filter for a specific route (e.g., Delhi → Mumbai) and compare average prices per airline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc6c5070",
      "metadata": {
        "id": "bc6c5070"
      },
      "outputs": [],
      "source": [
        "# Flight price analysis – skeleton code\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Replace with real path, e.g., 'datasets/flight_price.csv'\n",
        "try:\n",
        "    flights = pd.read_csv('flight_price.csv')\n",
        "    print(\"Loaded flight_price.csv successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(\"flight_price.csv not found. Using dummy data for demonstration.\")\n",
        "    flights = pd.DataFrame({\n",
        "        'days_left':   [30, 20, 10, 5, 2, 1],\n",
        "        'price':       [3000, 3200, 4000, 5500, 7000, 9000],\n",
        "        'airline':     ['AirA', 'AirA', 'AirA', 'AirB', 'AirB', 'AirC'],\n",
        "        'source':      ['Delhi']*6,\n",
        "        'destination': ['Mumbai']*6\n",
        "    })\n",
        "\n",
        "# (a) Relationship between days_left and price\n",
        "price_by_days = flights.groupby('days_left')['price'].mean().sort_index()\n",
        "print(\"Average price by days_left:\\n\", price_by_days)\n",
        "\n",
        "# (b) Compare prices across airlines for Delhi-Mumbai route\n",
        "route_filter = (flights['source'] == 'Delhi') & (flights['destination'] == 'Mumbai')\n",
        "route_data = flights[route_filter]\n",
        "\n",
        "avg_price_by_airline = route_data.groupby('airline')['price'].mean().sort_values()\n",
        "print(\"\\nAverage price for Delhi-Mumbai by airline:\\n\", avg_price_by_airline)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b3f1af",
      "metadata": {
        "id": "52b3f1af"
      },
      "source": [
        "### Interpretation (typical pattern)\n",
        "- **Days left vs. price:**  \n",
        "  - Prices often **start moderate**, may drop a bit in a sweet spot (e.g., 20–30 days before travel), and then **increase sharply** as the departure date approaches.  \n",
        "  - This can look almost **exponential** in the last few days.\n",
        "- **Best booking window:**  \n",
        "  - In many markets, booking roughly **2–4 weeks in advance** often gives a good trade‑off between price and flexibility (exact window depends on airline and route).\n",
        "- **Airlines comparison:**  \n",
        "  - Some airlines position themselves as **low‑cost carriers** (cheaper prices, fewer amenities).  \n",
        "  - Others are **premium airlines** (higher price, more legroom, free meals, better service).  \n",
        "  - We identify them by comparing **average route‑wise prices**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a5c32c3",
      "metadata": {
        "id": "7a5c32c3"
      },
      "source": [
        "## Question 10 – HR Analytics Dataset\n",
        "**(a)** What factors most strongly correlate with employee attrition? Use visualizations to show key drivers (e.g., satisfaction, overtime, salary).  \n",
        "**(b)** Are employees with more projects more likely to leave?  \n",
        "Dataset: `hr_analytics`  \n",
        "_(Include your Python code and output in the code box below.)_\n",
        "\n",
        "### Plan\n",
        "- Assume the dataset has columns like: `Attrition` (Yes/No or 1/0), `JobSatisfaction`, `MonthlyIncome`, `OverTime`, `NumProjects` (or similar).  \n",
        "- (a) Compute correlations between numeric features and Attrition (encoded as 0/1). Plot bar charts or boxplots.  \n",
        "- (b) Group by number of projects and compute attrition rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8f2254",
      "metadata": {
        "id": "7f8f2254"
      },
      "outputs": [],
      "source": [
        "# HR Analytics – skeleton code\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: Replace with actual path, e.g., 'hr_analytics.csv'\n",
        "try:\n",
        "    hr = pd.read_csv('hr_analytics.csv')\n",
        "    print(\"Loaded hr_analytics.csv successfully\")\n",
        "except FileNotFoundError:\n",
        "    print(\"hr_analytics.csv not found. Using dummy data for demonstration.\")\n",
        "    hr = pd.DataFrame({\n",
        "        'Attrition':        [1, 0, 1, 0, 1, 0],  # 1 = left, 0 = stayed\n",
        "        'JobSatisfaction':  [2, 4, 1, 3, 2, 4],\n",
        "        'MonthlyIncome':    [3000, 7000, 3500, 8000, 2800, 9000],\n",
        "        'OverTime':         ['Yes', 'No', 'Yes', 'No', 'Yes', 'No'],\n",
        "        'NumProjects':      [5, 3, 6, 2, 7, 3]\n",
        "    })\n",
        "\n",
        "# Convert Attrition to numeric if needed\n",
        "# Here we assume it is already 0/1. If it were 'Yes'/'No', we could map it:\n",
        "# hr['Attrition'] = hr['Attrition'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# (a) Correlation of numeric features with Attrition\n",
        "numeric_cols = hr.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "correlations = hr[numeric_cols].corr()['Attrition'].sort_values(ascending=False)\n",
        "print(\"Correlation of numeric features with Attrition:\\n\", correlations)\n",
        "\n",
        "# (b) Attrition vs number of projects\n",
        "attrition_by_projects = hr.groupby('NumProjects')['Attrition'].mean()\n",
        "print(\"\\nAttrition rate by NumProjects:\\n\", attrition_by_projects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c3682f",
      "metadata": {
        "id": "18c3682f"
      },
      "source": [
        "### Interpretation (example reasoning)\n",
        "- Factors often associated with **higher attrition** include:  \n",
        "  - **Low job satisfaction** (employees who are unhappy leave more).  \n",
        "  - **OverTime = Yes** (overworked employees may burn out).  \n",
        "  - **Very low or very high workloads** (too few or too many projects) depending on context.  \n",
        "  - **Lower salaries** compared to peers in similar roles.\n",
        "- For part (b), if we see that attrition rate increases with `NumProjects`, we can hypothesize that **excessive workload** is pushing people to leave. HR could respond by rebalancing workloads, hiring more staff, or improving work‑life balance.\n",
        "\n",
        "---\n",
        "This completes the detailed answers and example code for **all 10 questions** in a single notebook. Before submission, make sure to:\n",
        "1. Update file paths for the datasets on your machine.\n",
        "2. Re‑run the analysis cells so that outputs reflect the **real datasets** from the assignment.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}